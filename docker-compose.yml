version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    image: proxyai-api
    env_file: .env
    ports:
      - "8000:8000"
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    command:
      - uvicorn
      - main:app
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --workers
      - "1"                                # <- hard-code to avoid ${API_WORKERS} interpolation gotchas

  cpu-worker:
    build:
      context: .
      dockerfile: Dockerfile
    image: proxyai-cpu-worker
    env_file: .env
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    environment:
      HF_HOME: /hf_cache                 # <- force HF cache dir
      TRANSFORMERS_CACHE: /hf_cache
      HF_DATASETS_CACHE: /hf_cache
    command:
      - celery
      - -A
      - celery_app.celery_app
      - worker
      - --loglevel=info
      - --concurrency
      - "4"                               # or however many CPUs you want to dedicate
      - --hostname
      - cpu-worker
      - --queues
      - preprocess_cpu

  gpu-worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: proxyai-gpu-worker
    env_file: .env
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4g
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    environment:
      HF_HOME: /hf_cache
      TRANSFORMERS_CACHE: /hf_cache
      HF_DATASETS_CACHE: /hf_cache
    command:
      - celery
      - -A
      - celery_app.celery_app
      - worker
      - --loglevel=info
      - --concurrency
      - "1"                               # GPU concurrency = 1 to stay inside 4 GB
      - --hostname
      - gpu-worker
      - --queues
      - preprocess_gpu

  flower:
    image: mher/flower:latest
    env_file: .env
    ports:
      - "5555:5555"
    command:
      - flower
      - --broker=${CELERY_BROKER_URL}
      - --address=0.0.0.0
      - --port=5555

volumes:
  upload-data:
  hf_cache: