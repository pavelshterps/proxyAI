version: "3.8"

volumes:
  upload-data:
  hf_cache:

services:
  redis:
    image: redis:7.0-alpine
    restart: always

  tusd:
    image: tusproject/tusd:latest
    command: ["-port", "1080", "-upload-dir", "/files"]
    volumes:
      - upload-data:/files
    ports:
      - "1080:1080"

  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: proxyai-api
    restart: always
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    env_file: .env
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - tusd
    command: >
      uvicorn main:app
      --host 0.0.0.0
      --port 8000
      --workers ${API_WORKERS:-1}

  cpu-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: proxyai-cpu-worker
    restart: always
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    env_file: .env
    depends_on:
      - redis
    command: >
      celery -A celery_app.celery_app
      worker
      --loglevel=info
      --concurrency=${WORKER_CPU_CONCURRENCY:-4}
      --queues=preprocess_cpu

  gpu-worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: proxyai-gpu-worker
    restart: always
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    env_file: .env
    depends_on:
      - redis
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4G
    command: >
      celery -A celery_app.celery_app
      worker
      --loglevel=info
      --concurrency=${WORKER_GPU_CONCURRENCY:-1}
      --queues=preprocess_gpu

  flower:
    image: mher/flower:0.9.7
    container_name: proxyai-flower
    restart: always
    depends_on:
      - redis
    ports:
      - "5555:5555"
    command: >
      flower
      --app=celery_app.celery_app
      --port=5555