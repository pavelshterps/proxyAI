version: "3.8"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: proxyai-api
    env_file: .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${FASTAPI_PORT}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - upload-data:/tmp/uploads
      - results-data:/tmp/results
    depends_on:
      redis:
        condition: service_healthy
      tusd:
        condition: service_started
    ports:
      - "${FASTAPI_PORT}:8000"
    command: >
      uvicorn main:app
      --host 0.0.0.0
      --port ${FASTAPI_PORT}
      --workers ${API_WORKERS}

  cpu-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: proxyai-cpu-worker
    env_file: .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-lc", "celery -A celery_app status"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - upload-data:/tmp/uploads
      - results-data:/tmp/results
      - diarizer-cache:/tmp/diarizer_cache
    depends_on:
      redis:
        condition: service_healthy
    command: >
      celery -A celery_app worker
      --loglevel=info
      --concurrency ${CPU_CONCURRENCY}
      --hostname cpu-worker
      --queues preprocess_cpu

  gpu-worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: proxyai-gpu-worker
    env_file: .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-lc", "celery -A celery_app status"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - upload-data:/tmp/uploads
      - results-data:/tmp/results
      # Bind-маунт локального HF-кэша
      - /path/to/hf_cache_host:/hf_cache:rw
      - ctranslate2-cache:/root/.cache/ctranslate2
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: >
      celery -A celery_app worker
      --loglevel=info
      --concurrency ${GPU_CONCURRENCY}
      --hostname gpu-worker
      --queues preprocess_gpu

  redis:
    image: redis:7-alpine
    container_name: proxyai-redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    volumes:
      - redis-data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"

  tusd:
    image: tusproject/tusd:latest
    container_name: proxyai-tusd
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:1080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "1080:1080"
    command:
      - tusd
      - --host=0.0.0.0
      - --port=1080
      - --dir=/data
    volumes:
      - upload-data:/data

  flower:
    image: mher/flower
    container_name: proxyai-flower
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "5555:5555"
    command: >
      flower
      --broker=${CELERY_BROKER_URL}
      --port=5555
      --basic_auth=${FLOWER_USER}:${FLOWER_PASS}

volumes:
  upload-data:
  results-data:
  diarizer-cache:
  ctranslate2-cache:
  redis-data: