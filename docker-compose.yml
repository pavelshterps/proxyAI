version: '3.8'
services:
  api:
    image: proxyai-api
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    ports:
      - "8000:8000"
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    command:
      - uvicorn
      - main:app
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --workers
      - "${API_WORKERS}"

  cpu-worker:
    image: proxyai-cpu-worker
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    command:
      - celery
      - -A
      - celery_app.celery_app
      - worker
      - --loglevel=info
      - --concurrency
      - "${CELERY_CONCURRENCY}"
      - --hostname
      - cpu-worker
      - --queues
      - preprocess_cpu

  gpu-worker:
    image: proxyai-gpu-worker
    build:
      context: .
      dockerfile: Dockerfile.gpu
    env_file: .env
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4g
    volumes:
      - upload-data:/tmp/uploads
      - hf_cache:/hf_cache
    command:
      - celery
      - -A
      - celery_app.celery_app
      - worker
      - --loglevel=info
      - --concurrency
      - "1"
      - --hostname
      - gpu-worker
      - --queues
      - preprocess_gpu

  flower:
    image: mher/flower:latest
    env_file: .env
    ports:
      - "5555:5555"
    command:
      - flower
      - --broker=${CELERY_BROKER_URL}
      - --address=0.0.0.0
      - --port=5555

volumes:
  upload-data:
  hf_cache: