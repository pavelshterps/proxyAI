# Dockerfile.gpu

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Use a dedicated cache directory for Huggingface models
ENV HF_HOME=/hf_cache

# Install Python and FFmpeg
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
       python3.10 python3-pip ffmpeg \
  && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY requirements.txt .
RUN pip3 install --no-cache-dir --upgrade pip \
  && pip3 install --no-cache-dir -r requirements.txt

# Pre-compile Whisper model on CPU to generate CTranslate2 binary
RUN python3 - << 'EOF'
from faster_whisper import WhisperModel
WhisperModel(
    model_size_or_path="openai/whisper-large-v2",
    device="cpu",
    compute_type="float16",
    compile=True
)
EOF

# ---- Application code ----
# Copy configuration, code, and static frontend
COPY config        ./config
COPY main.py       ./main.py
COPY celery_app.py ./celery_app.py
COPY tasks.py      ./tasks.py
COPY static        ./static

ENV PYTHONUNBUFFERED=1