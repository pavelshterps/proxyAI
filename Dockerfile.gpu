FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Use a dedicated cache directory for Huggingface models
ENV HF_HOME=/hf_cache

# Install Python and FFmpeg
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
       python3.10 python3-pip ffmpeg \
  && rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir transformers

WORKDIR /app
COPY requirements.txt .
RUN pip3 install --no-cache-dir --upgrade pip \
    && pip3 install --no-cache-dir ctranslate2[cuda] \
    && pip3 install --no-cache-dir -r requirements.txt

# Patch CTranslate2 converter to import transformers using dynamic path
RUN TRANSFORMER_FILE=$(python3 -c "import ctranslate2, os; print(os.path.dirname(ctranslate2.__file__) + '/converters/transformers.py')") \
    && sed -i '1i import transformers' "$TRANSFORMER_FILE"

# Pre-quantize whisper-medium into CTranslate2 format
RUN ct2-transformers-converter \
    --model openai/whisper-medium \
    --quantization float16 \
    --output_dir /hf_cache/quantized/whisper-medium-float16

# ---- Application code ----
COPY config        ./config
COPY main.py       ./main.py
COPY celery_app.py ./celery_app.py
COPY tasks.py      ./tasks.py
COPY static        ./static

ENV PYTHONUNBUFFERED=1